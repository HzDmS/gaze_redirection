# Photo-realistic Monocular Gaze Redirection using Generative Adversarial Networks

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[[Paper](https://arxiv.org/abs/1903.12530)] [[Video](https://youtu.be/SJBb9t6KmDY)]

Authors: **Zhe He**, Adrian Spurr, Xucong Zhang, Otmar Hilliges

Contact: zhehe@student.ethz.ch

<p align="center">
  <img src="/imgs/framework.jpg">
</p>

The following gifs are made of images generated by our method. For each one, the input is a still image.

<p align="center">
<img src="/imgs/circle.gif" width="100" height="100" /> &ensp; <img src="/imgs/zed.gif" width="100" height="100" /> &ensp; <img src="/imgs/horizontal.gif" width="100" height="100" /> &ensp; <img src="/imgs/vertical.gif" width="100" height="100" />
</p>

Our method is also capable of handling different head poses.

<p align="center">
  <img src="/imgs/headpose.jpg">
</p>

## Note

The code here is the development version. It can be used for training, but there might be some redundant code and compatiblity issues. The final version will be released soon.

## Dependencies

 tensorflow == 1.7  
 numpy == 1.13.1  
 scipy == 0.19.1  

## Dataset

The dataset contains eye patch images parsed from [Columbia Gaze Dataset](http://www.cs.columbia.edu/~brian/projects/columbia_gaze.html).

```Bash
tar -xvf dataset.tar
```

## VGG-16 pretrained weights

```Bash
wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz .
tar -xvf vgg_16_2016_08_28.tar.gz
```

## Train

```Bash
python main.py --mode train --data_path ./dataset/ --log_dir ./log/ --batch_size 32 --vgg_path ./vgg_16.ckpt
```

## Test

```Bash
python main.py --mode eval --data_path ./dataset/ --log_dir ./log/ --batch_size 21
```

Then, a folder named **eval** will be generated in folder **./log/**. Generated images, input images and target images will be stored in **quanti_eval/**.
